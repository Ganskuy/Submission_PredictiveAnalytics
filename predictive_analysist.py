# -*- coding: utf-8 -*-
"""predictive_analysist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1__WqWH5oECTL14kVLP7nl6fMJ1xqpkcI

# Predictive Analytics : Pengaruh GDP Suatu Negara terhadap Demografi, dan Kesehatan Warga

## Business Understanding

### Problem Statement

Rumusan masalah dari topik yang diangkat adalah :
- Dari berbagai fitur yang ada, fitur manakah yang paling berpengaruh terhadap Angka Harapan Hidup (Life Expectancy) seseorang?
- Bagaimana mengetahui pengaruh dari riwayat dari fitur-fitur yang ada terhadap Angka Harapan Hidup (Life Expctancy) Seseorang?

### Goals

Tujuan dari menyelesaikan permasalahan diatas adalah :
- Mengetahui fitur apa sajakah yang memiliki korelasi paling tinggi terhadap Angka Harapan Hidup (Life Expectancy) seseorang
- Membuat model machine learning yang dapat memprediksi pengaruh dari riwayat fitur-fitur yang ada terhadap besar Angka Harapan Hidup (Life Expectancy) seseorang

### Solution Statement

- Melakukan analisa pada data untuk memahami fitur-fitur yang mempengaruhi Angka Harapan Hidup (Life Expectancy) seseorang, dengan menerapkan visualisasi pada data untuk mengetahui korelasi antar fitur
- Menggunakan beberapa algoritma machine learning berbeda untuk membandingkan performa tiap modelnya, hal ini bertujuan untuk mendapatkan algoritma yang memiliki akurasi paling tinggi dalam memperkirakan pengaruh dari fitur-fitur yang ada terhadap Angka Harapan Hidup (Life Expectancy) Seseorang.

### Metodologi

tujuan yang ingin dicapai dari proyek ini adalah, memprediksi besar angka harapan hidup seseorang terhadap gaya hidup ataupun penyakit yang dimiliki. Angka harapan hidup merupakan vaiabel kontinu, dan ketika kita membuat prediksi dengan variable kontinu pada predictive analytics, artinya kita sedang menyelesaikan permasalahan regresi. Oleh karena itu, metolodogi pada proyek ini adalah membangun model regresi dengan angka harapan hidup (Life expectancy) sebagai targetnya.

### Metrik

Indikator yang saya gunakan untuk menilai seberapa efektif model regresi dalam memprediksi variabel kontinu, seperti Angka Harapan Hidup, adalah Mean Squared Error (MSE). Metrik ini berfungsi untuk menilai seberapa tepat prediksi model dibandingkan dengan nilai sebenarnya, dimana MSE memberikan penalti lebih besar untuk kesalahan yang lebih signifikan.

### Data Understanding

Data Understanding merupakan proses pada analisia data yang bertujuan untuk memahami dataset secara mendalam sebelum melakukan analisis lebih lanjut.

### Data Loading

Data Loading merupakan proses memuat dataset yang akan digunakan untuk membangun model machine learning

#### Informasi Dataset

| **Jenis**        | **Keterangan**                                                                         |
|------------------|----------------------------------------------------------------------------------------|
| **Title**        | Health and Demographics Dataset                                                        |
| **Source**       | [Kaggle](https://www.kaggle.com/datasets/uom190346a/health-and-demographics-dataset)   |                                               
| **Owner**        | [Laksika Tharmalingam](https://www.kaggle.com/uom190346a)                              |
| **License**      | Database: Open Database, Contents                                                      |
| **Visibility**   | Publik                                                                                 |
| **Tags**         | Social Science                                                                         |
| **Usability**    | 10.00                                                                                  |
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

df=pd.read_csv('Life_Expectancy_Data.csv')
df

"""Dari output diatas, terlihat bahwa dataset terdiri dari 1649 baris dan 22 kolom

### Exploratory Data Analysis (EDA) - Eksplorasi data

Berikut adalah informasi mengenai variabel-variabel pada dataset yang didapat dari kaggle :

- Country: Negara dimana data diambil.
- Year: Tahun dimana data diambil.
- Status: Penjelasan mengenai negara tersebut Developing (berkembang) atau Developed (maju).
- Life Expectancy: Angka Harapan Hidup.
- Adult Mortality: Mengukur probabilitas kelangsungan hidup antara usia 15 hingga 60 per 1.000 penduduk..
- Infant Deaths: kesehatan bayi dengan melihat jumlah kematian bayi per 1.000 kelahiran hidup.
- Alcohol: rata-rata konsumsi alkohol dalam liter per kapita.
- Percentage Expenditure: pengeluaran kesehatan sebagai persentase dari GDP suatu negara.
- Hepatitis B:  cakupan imunisasi untuk Hepatitis B.
- Measles: dampak penyakit yang dapat dicegah melalui jumlah kasus yang dilaporkan per 1.000 penduduk.
- BMI:  rata-rata Indeks Massa Tubuh.
- Under-Five Deaths:  angka kematian anak dengan jumlah kematian di bawah usia lima tahun per 1.000 kelahiran hidup.
- Polio: cakupan imunisasi untuk Polio.
- Total Expenditure:  total pengeluaran kesehatan sebagai persentase GDP.
- Diphtheria: cakupan imunisasi untuk Diphteria.
- HIV/AIDS: prevalensi HIV/AIDS sebagai persentase populasi.
- GDP: data Produk Domestik Bruto.
- Population: populasi suatu negara.
- Thinness 1-19 Years: prevalensi anak-anak dan remaja berusia 1-19 tahun yang kurus.
- Thinness 5-9 Years: prevalensi anak-anak dan remaja berusia 5-9 tahun yang kurus.
- Income Composition of Resources: indeks komposit yang mencerminkan distribusi pendapatan dan akses sumber daya.
- Schooling: data rata-rata tahun sekolah.
"""

df.info()

"""Dari output tersebut dikethaui bahwa :
- terdapat 2 kolom dengan data bertipe string
- terdapat 8 kolom dengan data bertipe integer
- terdapat 12 kolom dengan data bertipe float
"""

df.describe()

"""Fungsi diatas menjelaskan informasi statistik dengan penjelasan sebagai berikut :

- Count adalah jumlah sampel pada data.
- Mean adalah nilai rata-rata.
- Std adalah standar deviasi.
- Min yaitu nilai minimum setiap kolom.
- 25% adalah kuartil pertama. Kuartil adalah nilai yang menandai batas interval dalam empat bagian sebaran yang sama.
- 50% adalah kuartil kedua, atau biasa juga disebut median (nilai tengah). - 75% adalah kuartil ketiga.
- Max adalah nilai maksimum.
"""

df.shape

df.columns = df.columns.str.strip()

"""disini saya menghapus spasi pada bagian belakang kolom

### EDA - Menangani Missing Value, Data Duplikat, dan Outliers
"""

df.duplicated().sum()

"""Terlihat bahwa tidak terdapat data duplikat pada dataset"""

df.isnull().sum()

"""Terlihat juga bahwa tidak terdapat missing value pada dataset"""

for col in df.columns:
    jumlah_nol = (df[col] == 0).sum()
    print(f"Nilai 0 di kolom {col} ada: {jumlah_nol}")

"""Dapat dilihat bahwa terdapat nilai 0 pada kolom :
- Infant deaths
- Percentage expenditure
- Measles
- under-five deaths
- Income composition of resources

Namun disini saya tidak bisa menghapus nilai 0 tersebut, karena kolom yang berisi nilai 0 paling banyak adalah 554 kolom, yang artinya saya akan menhaous hampir 50% dari keseluruhan dataset, oleh karena itu saya hanya akan mengisi nilai 0 tersebut dengan mean, pada data yang tidak masuk akal jika berisi nilai 0 seperi :
- Percentage expenditure
- Income composition of resources

sedangkan pada fitur lainnya akan saya biarkan bernilai 0, karena jika saya isi dengan median, hal tersebut akan mengacaukan informasi pada fitur tersebut, contohnya seperti, jika saya mengisi mean pada data infant deaths, maka hal tersebut akan mengacaukan informasi pada data, karena memang mungkin saja terjadi tidak adanya kematian pada bayi ketika data tersebut diambil.
"""

df.loc[(df['Measles']==0)]

"""disini saya mendapati bahwa, kolom yang berisi nilai 0 terlalu banyak, sehingga jika saya drop akan mengurangi dimensi dari dataset dengan sangat besar, sehingga saya akan mengisinya dengan mean, namun saya hanya akan mengisi kolom percentage expenditure dan income composition of resources, sedangkan pada kolom infant death, measles, under-five death akan saya biarkan 0 karena nilai 0 pada kolom tersebut memanglah masuk akal dan akan mempengaruhi hasil prediksi jika saya isi"""

df_cl=df.copy()
df_cl['percentage expenditure'] = df_cl['percentage expenditure'].replace(0, df_cl['percentage expenditure'].mean())
df_cl['Income composition of resources'] = df_cl['Income composition of resources'].replace(0, df_cl['Income composition of resources'].mean())

"""disini saya engisi data dengan nilai 0 dengan mean, hal ini dilakukan karena jika saya drop kolom-kolom tersebut, akan mereduksi dimensi dari dataset dengan sangat signifikan"""

for col in df_cl.columns:
    jumlah_nol = (df_cl[col] == 0).sum()
    print(f"Nilai 0 di kolom {col} ada: {jumlah_nol}")

"""terlihat bahwa nilai 0 pada fitur percentage expenditure dan income composition of resources sudah tidak memiliki nilai 0

#### Menangani Outliers
"""

hist = df_cl.select_dtypes(include=['number']).columns
plt.figure(figsize=(15, 10))
for i, col in enumerate(hist, 1):
    plt.subplot((len(hist) // 3) + 1, 3, i)
    sns.boxplot(x=df_cl[col])
    plt.title(f'Boxplot of {col}')
    plt.tight_layout()

plt.show()

"""Terlihat ada banyak outlier pada banyak fitur, kita akan menanganinya menggunakan IQR Method"""

dia_clean=df_cl.copy()
# Hitung Q1, Q3, dan IQR hanya untuk kolom numerikal
Q1 = dia_clean[hist].quantile(0.25)
Q3 = dia_clean[hist].quantile(0.75)
IQR = Q3 - Q1
# Buat filter untuk menghapus baris yang mengandung outlier di kolom numerikal
filter_outliers = ~((dia_clean[hist] < (Q1 - 1.5 * IQR)) |
                    (dia_clean[hist] > (Q3 + 1.5 * IQR))).any(axis=1)
# Terapkan filter ke dataset asli (termasuk kolom non-numerikal)
dia_clean = dia_clean[filter_outliers]
# Cek ukuran dataset setelah outlier dihapus
dia_clean.shape

"""Dataset kini menjadi 669 baris setelah outlier dihapus

### EDA - Univariate Analysis
"""

numerical_features = hist
categorical_features = ['Country', 'Status']

feature = categorical_features[0]
count = dia_clean[feature].value_counts()
percent = 100 * dia_clean[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel': count, 'persentase': percent.round(1)})
print(df)

# Set figure size
plt.figure(figsize=(15, 6))  # adjust width and height as needed
count.plot(kind='bar', title=feature)
plt.xlabel(feature)
plt.ylabel("Jumlah Sampel")
plt.tight_layout()
plt.show()

"""dari visualisasi tersebut, kita dapat menyimpulkan bahwa, sebagian besar sampel berasal dari negara Albania dengan persentase tertinggi, yaitu sebesar 2.4%, dan sampel paling sedikit berasa dari negara Sri Lanka, Haiti, Guinea, Turkey, dan Croatia dengan masing-masingnya memiliki persentase sebesar 0.1%"""

feature = categorical_features[1]
count = dia_clean[feature].value_counts()
percent = 100 * dia_clean[feature].value_counts(normalize=True)
df = pd.DataFrame({'jumlah sampel': count, 'persentase': percent.round(1)})
print(df)

# Set figure size
plt.figure(figsize=(15, 6))  # adjust width and height as needed
count.plot(kind='bar', title=feature)
plt.xlabel(feature)
plt.ylabel("Jumlah Sampel")
plt.tight_layout()
plt.show()

"""dari visualisasi tersebut, kita dapat menyimpulkan bahwa, sebagian besar negara dari sampe yang diambil merupakan negara berkembang (developing), dan sebagian kecilnya berasal dari negara maju (developed)"""

dia_clean.hist(bins=50, figsize=(20,15))
plt.show()

"""dari visualisasi diatas, kita akan berfokus pada fitur target dari proyek ini, yaitu life expectancy, dan kita bisa menyimpulkan beberapa hal :
- Angka Life Expectancy meningkat seiring dengan bertambahnya jumlah sampel
- hanya sedikit sampel yang memiliki life expectancy diatas 65 tahun
- dapat dilihat dari sampel tersebut bahwa, rata-rata life expectancy berada di sekitar 25-55 tahun
- Distribusi Life Expectancy cenderung miring ke kanan (left-skewed), yang menunjukkan bahwa nilai median (nilai tengah) berada di kuartil ketiga dan lebih tinggi daripada nilai rata-rata.

### EDA - Multivariate Analysis
"""

cat_features = dia_clean.select_dtypes(include='object').columns.to_list()

for col in cat_features:
    g = sns.catplot(
        x=col,
        y="Life expectancy",
        hue=col,
        kind="bar",
        dodge=False,
        height=5,      # set height of each plot
        aspect=4,      # set width relative to height
        data=dia_clean,
        palette="Set3",
        legend=False
    )
    g.fig.suptitle(f"Rata-rata 'Life Expectany' Relatif terhadap - {col}", y=1.02)  # move title up
    plt.tight_layout()
    plt.show()

"""Dari visualisasi diatas, kita mendapati bahwa :
- Pada visualisasi Rata-rata life expectany terhadap country, terlihat bahwa life expectancy pada negara-negara yang terdapat pada dataset, cukup tinggi yaitu diatas 65 tahun
- Pada visualisasi Rata-rata life expectany terhadap status, terlihat bahwa life expectancy pada negara-negara maju (developed) cenderung lebih tinggi ketimbang life expectancy pada negara berkembang (developing)
"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(dia_clean, diag_kind = 'kde')

"""dari visualisasi diatas, dapat dilihat bahwa :
- Income Composition of Resources vs Life Expectancy : Terlihat adanya korelasi positif yang cukup kuat disini, bahwa semakin tinggi distribusi pendapatan dan fasilitas, semakin tinggi juga life expectancy nya
- Schooling vs Life Expectancy : Terlihat juga adanya korelasi positif yang kuat disini, bahwa semakin tinggi pendidikan yang ditempuh, semakin tinggi juga life expectancynya
"""

plt.figure(figsize=(15, 8))
correlation_matrix = dia_clean[numerical_features].corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""dari correlation matrix diatas, kita dapat mengetahui bahwa :
- Income Composition of Resources memiliki korelasi yang kuat terhadap Life Expectancy, Fitur ini juga memiliki korelas yang kuat terhadap Schooling, hal ini cukup relevan mengingat orang yang mendapatkan fasilitas yang mendukung pasti akan menempuh pendidikan setinggi mungkin.
- Schooling terlihat memiliki korelasi yang kuat dengan Life Expectancy, fitur ini juga memiliki korelasi yang cukup kuat terhadap schooling, hal ini cukup relevan mengingat kesimpulan yang kita dapat sebelumnya, orang yang mendapatkan fasilitas yang mendukung pasti akan menempuh pendidikan setinggi mungkin.

Kesimpulan diatas juga menjawab pertanyaan kita diatas mengenai "fitur apa sajakah yang memiliki korelasi paling kuat dengan life expectancy" bahwa, Fasilitas yang didapat seseorang serta tingkat pendidikan seseorang memiliki korelasi yang kuat terhadap Life Expectancy, hal ini terbilang cukup relevan mengingat, seseorang yang menempuh pendidikan tinggi cenderung memiliki life expectancy yang tinggi pula karena tingginya mimpi yang biasanya dimiliki oleh seorang pelajar, dan seseorang yang mendapatkan fasilitas yang mendukung juga memiliki life expectancy yang tinggi
"""

dia_clean.drop([
    'Year', 'Adult Mortality', 'infant deaths', 'Measles',
    'under-five deaths', 'HIV/AIDS', 'Population',
    'thinness  1-19 years',
    'thinness 5-9 years'
], inplace=True, axis=1)
dia_clean.head()

dia_clean.shape

"""Disini saya menghapus kolom-kolom yang memiliki korelasi rendah dengan Life Expectancy

### Data Preparation - Encding Fitur Category
"""

from sklearn.preprocessing import  OneHotEncoder

status_dummies = pd.get_dummies(dia_clean['Status'], prefix='Status', dtype=int)
dia_clean = pd.concat([dia_clean, status_dummies], axis=1)
dia_clean.drop(['Status'], axis=1, inplace=True)
dia_clean.head()

"""Disini saya melakukan encoding pada fitur Status menggunakan One Hot Encoder, hal ini dilakukan karena Status hanya berisi 2 kemungkinan, yaitu Developing dan Deleoped, hal ini cocok dengan One Hot Encoder yang melakukan encoding pada data didalam fitur kedalam binary"""

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
dia_clean['Country'] = label_encoder.fit_transform(dia_clean['Country'])
dia_clean.head()

"""Pada fitur Country, saya melakukan Encoding menggunakan Label Encoder, hal ini dilakukan karena fitur country ini berisi banyak sekali nama negara, sehingga label encoder lebih cocok karena tidak akan membuat dimensi dataset menjadi sangat besar

### Data Preparation - Splitting
"""

from sklearn.model_selection import train_test_split

X = dia_clean.drop(["Life expectancy"],axis =1)
y = dia_clean["Life expectancy"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

"""disini saya menggunakan fitur Life Expectancy sebagai target, dan saya menggunakan pembagian data 90:10 antara data latih dan data uji"""

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""### Standarisasi"""

from sklearn.preprocessing import StandardScaler

numerical_features = ['Alcohol', 'percentage expenditure', 'Hepatitis B', 'BMI', 'Polio', 'Total expenditure', 'Diphtheria', 'GDP', 'Income composition of resources', 'Schooling']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

X_train[numerical_features].describe().round(4)

"""Algoritma machine learning akan menghasilkan performa yang lebih baik jika data yang digunakan memiliki skala yang relatif sama atau terdistribusi normal. Standarisasi mengubah skala fitur numerik agar memiliki distribusi standar, dengan standar deviasi 1 dan mean 0

### Model Development

Pada Modelling, saya menggunakan 3 algoritma berbeda yaitu :
- K-Means
- Random Forest
- Ada Boost
"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""### K-Means"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, y_train)

models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

"""Pada algortima K-Means :
- Menggunakan MSE sebagai metrik evaluasi
- n=10 berarti, model akan mencari 10 tetangga terdekat dari titik data baru dan mengambil rata-rata dari target mereka untuk menghasilkan prediksi.

### Rabdom Forest
"""

# Impor library yang dibutuhkan
from sklearn.ensemble import RandomForestRegressor

# buat model prediksi
RF = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)

models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

"""Pada algoritma Random Forest :
- Menggunakan MSE sebagai metrik evaluasi
- n_estimators=50 berarti, jumlah pohon yang dibangun sebanyak 50
- max_depth=16 berarti, batas kedalam maksimum tiap pohon adalah 16 level, hal ini dilakukan untuk mencegah overfitting
- n_jobs=-1 berarti, Menggunakan seluruh core CPU yang tersedia untuk mempercepat training

### Boosting
"""

from sklearn.ensemble import AdaBoostRegressor

boosting = AdaBoostRegressor(learning_rate=0.05, random_state=55)
boosting.fit(X_train, y_train)
models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)

"""Pada algoritma Ada Boost :
- Menggunakan MSE sebagai metrik evaluasi
- learning_rate=0.05 berarti, Mengontrol kontribusi masing-masing model lemah. Nilai yang kecil berarti model belajar lebih lambat tapi lebih stabil.

### Evaluasi Model
"""

X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

# Panggil mse
mse

"""Terlihat bahwa pada setiap algoritma memiliki nilai MSE yang relatif kecil"""

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

prediksi = X_test.iloc[:1].copy()
pred_dict = {'y_true':y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)

"""Dari tabel di atas, dapat dilihat bahwa setiap model menghasilkan prediksi yang bervariasi untuk setiap nilai aktual (y_true). Model K-Nearest Neighbor (KNN) menunjukan hasil yang paling mendekati nilai aktual, sementara Random Forest dan Ada Boost menunjukkan hasil yang kompetitif. Secara keseluruhan, performa model dapat bervariasi tergantung pada karakteristik data dan hubungan yang ada antara fitur dan target."""